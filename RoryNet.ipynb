{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PHastings37/Mphys-proj/blob/main/RoryNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To do list\n",
        "1. Figure out normalization when creating the datasets\n",
        "2. Get to work with tensorboard to see if weights are changing\n",
        "3. Issue: currently the network reaches approx minimum loss after just 7 batches of 1 epoch. This may be because the network is very simple or other factors."
      ],
      "metadata": {
        "id": "e9Hd9ZIlACIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfWFoJBL2P6A",
        "outputId": "d2bd3ff1-0b27-4b69-b62e-3cf60fa8ee82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install opencv-contrib-python\n",
        "!pip install scikit-learn\n",
        "!pip install SimpleITK\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv3d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch import flatten\n",
        "from torch import nn\n",
        "from torch import reshape\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.optim import Adam\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "tb = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95pHZBRQ2Zbl",
        "outputId": "0849d62d-bc19-4d8d-982e-753038af429e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BNM-ezKVhmp",
        "outputId": "c9c721d0-c2d0-40c9-8fae-be27e3ae10dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-zAX1vZ2bSU",
        "outputId": "36866e46-30ac-4199-936d-b44c88ce9582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "/content/gdrive/My Drive/Mphys project/MPhys/Data/COLAB-Clinical-Data.csv\n"
          ]
        }
      ],
      "source": [
        "# Connect to GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')\n",
        "# /content/gdrive/MyDrive/MPhys/Data/COLAB-Clinical-Data.csv\n",
        "# Specify project folder location\n",
        "project_folder = \"/content/gdrive/My Drive/Mphys project/MPhys/Data\"\n",
        "clinical_data_filename = \"COLAB-Clinical-Data.csv\"\n",
        "print(os.path.join(project_folder, clinical_data_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HU0Og4e2k_k",
        "outputId": "715b38cc-2e2b-4204-c233-bc863f32838e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadata_file path: /content/gdrive/My Drive/Mphys project/MPhys/Data/COLAB-Clinical-Data.csv\n",
            "Length of metadata array is 100\n",
            "Dead counter after 547.5 days: 60\n",
            "Alive counter after 547.5 days: 40\n",
            "No-info counter after 547.5 days: 0\n",
            "The alive and dead arrays have been sorted (randomly) so that they are both of length 40, 40\n",
            "56\n"
          ]
        }
      ],
      "source": [
        "def equalise_array_lengths(array_1, array_2) :\n",
        "  \"\"\"\n",
        "  This functions takes in the arguments of two lists and makes sure they are returned as the same length.\n",
        "\n",
        "  Rory Farwell 02/12/2021\n",
        "  \"\"\"\n",
        "  # output_array = []\n",
        "  if len(array_1) > len(array_2) :\n",
        "    array_1 = array_1[:len(array_2)]\n",
        "  elif len(array_1) < len(array_2) :\n",
        "    array_2 = array_2[:len(array_1)]\n",
        "  #print(np.vstack((array_1, array_2)))\n",
        "  # output_array.append(array_1)\n",
        "  # output_array.append(array_2)\n",
        "  return (array_1, array_2)\n",
        "\n",
        "def remove_same_elements(small_array, long_array) :\n",
        "  \"\"\"\n",
        "  For use in the context, all the elements in small_array come from long_array.\n",
        "  This function will remove all of the elements used in small_array from_long_array.  \n",
        "  \"\"\"\n",
        "  for element in small_array :\n",
        "    long_array.remove(element)\n",
        "  return long_array\n",
        "\n",
        "def create_subgroup(input_array, original_array_length, desired_percentage) :\n",
        "  \"\"\"\n",
        "  This function outputs a subgroup array (e.g. training array) using a specified output array name,\n",
        "  input array and percentage length\n",
        "  \"\"\"\n",
        "  desired_length = int(original_array_length * desired_percentage)\n",
        "  output_array = random.sample(input_array, desired_length)\n",
        "  return output_array\n",
        "  \n",
        "\n",
        "# Open the metadata.csv file, convert to an array, and remove column headers\n",
        "metadata_file = os.path.join(project_folder, clinical_data_filename)\n",
        "print(f'metadata_file path: {metadata_file}')\n",
        "metadata = np.genfromtxt(metadata_file, comments = '%', dtype=\"str\", delimiter=\",\")\n",
        "print(f\"Length of metadata array is {len(metadata)}\")\n",
        "\n",
        "outcome_type = 1 #int(input(\"Select which outcome you are aiming to predict \\n(1=Locoregional, 2=Distant Metastasis, 3=Death):\"))\n",
        "check_day = 3000 #int(input(\"Select the number of days at which to check for event:\"))\n",
        "which_patients = 1 #int(input(\"Do you want to include patients whose last follow up is before the check day? (no = 0, yes = 1):\"))\n",
        "patient_with_event = []\n",
        "patient_no_event = []\n",
        "outcomes_train = []\n",
        "outcomes_test = []\n",
        "images = []\n",
        "\n",
        "check_day = 365 * 1.5 # This is defining the timeframe for which our CNN will consider the binary output (in days)\n",
        "\n",
        "patient_IDs = metadata[:,0] # selecting patient IDs from the csv file\n",
        "time_markers = metadata[:,8] # selecting the day of the last patient review from the csv file\n",
        "dead_statuses = metadata[:,9] # selecting the dead status on the last review day\n",
        "\n",
        "time_markers = time_markers.astype(np.float32) # converting to float\n",
        "dead_statuses = dead_statuses.astype(np.float32) # converting to float\n",
        "\n",
        "\n",
        "# Empty array which will be appended to later on\n",
        "# This array will contain the dead statuses of patients on the check day\n",
        "check_day_dead_statuses = [] \n",
        "\n",
        "# sanity check to check progress\n",
        "counter = 0 \n",
        "\n",
        "# A counter of howe many patients are dead, alive or censored at the check day\n",
        "dead_counter = 0\n",
        "alive_counter = 0\n",
        "no_info_counter = 0\n",
        "\n",
        "# Empty arrays that will be appended to.\n",
        "# They will be appended with the patient IDs and dead statuses on the check day\n",
        "dead_patient_array = [] \n",
        "alive_patient_array = []\n",
        "\n",
        "# The below 'for' loop will check patient status on the check day and convert if necessary.\n",
        "# Possibilites are:\n",
        "# 1) If the patient is dead and the last review point is after than the check day then\n",
        "# the patient is alive in the check day and so the dead status is changed in the new array\n",
        "# 2) If the patient is dead and the last review point is before the check day then the patient\n",
        "# is dead on the check day and their dead status is unchanged\n",
        "# 3) If the patient is alive and their last review point is after the check day then they\n",
        "# are alive at the check day and so their dead status remains unchanged\n",
        "# 4) If the patient has status alive and their last review point is before the \n",
        "# check day then we don't know if they are alive at the check day so they are ignored from the list.\n",
        "for i in range(len(dead_statuses)) :\n",
        "  temp_dead_status = dead_statuses[i]\n",
        "  temp_time_marker = time_markers[i]\n",
        "  if temp_dead_status == 1 : #if the patient is dead\n",
        "    if temp_time_marker < check_day :\n",
        "      check_day_dead_statuses.append(1) #confirms that the patient was dead after time 'check_day'\n",
        "      dead_patient_array.append([patient_IDs[i], 1])\n",
        "      dead_counter += 1\n",
        "      continue\n",
        "    elif temp_time_marker > check_day :\n",
        "      check_day_dead_statuses.append(0)\n",
        "      alive_patient_array.append([patient_IDs[i], 0])\n",
        "      alive_counter += 1\n",
        "      continue\n",
        "  elif temp_dead_status == 0 : #if the patient is alive\n",
        "    if temp_time_marker < check_day :\n",
        "      no_info_counter += 1\n",
        "      continue\n",
        "    elif temp_time_marker > check_day :\n",
        "      check_day_dead_statuses.append(0)\n",
        "      alive_patient_array.append([patient_IDs[i], 0])\n",
        "      alive_counter += 1\n",
        "      continue\n",
        "\n",
        "# Printing the results of this 'for' loop (the number of dead and alive patients at the check day)\n",
        "print(f\"Dead counter after {check_day} days: {dead_counter}\")\n",
        "print(f\"Alive counter after {check_day} days: {alive_counter}\")\n",
        "print(f\"No-info counter after {check_day} days: {no_info_counter}\")\n",
        "\n",
        "# Creating empty arrays that will be appended to later\n",
        "# These will contain the patient ID and dead status (on the check day).\n",
        "training_array = []\n",
        "testing_array = []\n",
        "validation_array = []\n",
        "\n",
        "# print(f'Unshuffled dead_patient_array = {dead_patient_array}')\n",
        "# print(f'Unshuffled alive_patient_array = {alive_patient_array}')\n",
        "\n",
        "# Shuffle both arrays to ensure a random selection of patient data which will be input to the CNN\n",
        "random.shuffle(dead_patient_array)\n",
        "random.shuffle(alive_patient_array)\n",
        "\n",
        "# print(f'Shuffled dead_patient_array = {dead_patient_array}')\n",
        "# print(f'Shuffled alive_patient_array = {alive_patient_array}')\n",
        "\n",
        "# print(equalise_array_lengths(dead_patient_array, alive_patient_array)[0])\n",
        "\n",
        "# Equalising the length of the 'dead' and 'alive' arrays so that we can ensure optimum training proportions\n",
        "new_dead_patient_array = equalise_array_lengths(dead_patient_array, alive_patient_array)[0]\n",
        "new_alive_patient_array = equalise_array_lengths(dead_patient_array, alive_patient_array)[1]\n",
        "print(f\"The alive and dead arrays have been sorted (randomly) so that they are both of length {len(new_dead_patient_array)}, {len(new_alive_patient_array)}\")\n",
        "\n",
        "# print(new_dead_patient_array)\n",
        "# print(new_alive_patient_array)\n",
        "# print(len(new_dead_patient_array))\n",
        "# print(len(new_alive_patient_array))\n",
        "\n",
        "equalised_array_length = len(new_alive_patient_array)\n",
        "\n",
        "train_patients_dead = create_subgroup(new_dead_patient_array, equalised_array_length, 0.7)\n",
        "train_patients_alive = create_subgroup(new_alive_patient_array, equalised_array_length, 0.7)\n",
        "# print(len(train_patients_dead))\n",
        "# print(len(train_patients_alive))\n",
        "\n",
        "new_dead_patient_array = remove_same_elements(train_patients_dead, new_dead_patient_array)\n",
        "new_alive_patient_array = remove_same_elements(train_patients_alive, new_alive_patient_array)\n",
        "# print(len(new_dead_patient_array))\n",
        "# print(len(new_alive_patient_array))\n",
        "\n",
        "test_patients_dead = create_subgroup(new_dead_patient_array, equalised_array_length, 0.15)\n",
        "test_patients_alive = create_subgroup(new_alive_patient_array, equalised_array_length, 0.15)\n",
        "# print(len(test_patients_dead))\n",
        "# print(len(test_patients_alive))\n",
        "\n",
        "new_dead_patient_array = remove_same_elements(test_patients_dead, new_dead_patient_array)\n",
        "new_alive_patient_array = remove_same_elements(test_patients_alive, new_alive_patient_array)\n",
        "# print(len(new_dead_patient_array))\n",
        "# print(len(new_alive_patient_array))\n",
        "\n",
        "validate_patients_dead = create_subgroup(new_dead_patient_array, equalised_array_length, 0.15)\n",
        "validate_patients_alive = create_subgroup(new_alive_patient_array, equalised_array_length, 0.15)\n",
        "# print(len(validate_patients_dead))\n",
        "# print(len(validate_patients_alive))\n",
        "\n",
        "new_dead_patient_array = remove_same_elements(validate_patients_dead, new_dead_patient_array)\n",
        "new_alive_patient_array = remove_same_elements(validate_patients_alive, new_alive_patient_array)\n",
        "# print(len(new_dead_patient_array))\n",
        "# print(len(new_alive_patient_array))\n",
        "\n",
        "\n",
        "outcomes_train = train_patients_dead + train_patients_alive\n",
        "outcomes_test = test_patients_dead + test_patients_alive\n",
        "outcomes_validate = validate_patients_dead + validate_patients_alive\n",
        "\n",
        "\n",
        "print(len(outcomes_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qt8pEoBq20QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62773c9c-436f-4b33-e500-1a947ef41bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1024., dtype=torch.float64)\n",
            "tensor(-1020.9913, dtype=torch.float64) tensor(54.8529, dtype=torch.float64)\n",
            "1\n",
            "tensor(-1024., dtype=torch.float64)\n",
            "tensor(-1020.7112, dtype=torch.float64) tensor(57.4263, dtype=torch.float64)\n",
            "1\n",
            "tensor(0.9452, dtype=torch.float64)\n",
            "tensor(1.0000, dtype=torch.float64) tensor(1.0000, dtype=torch.float64)\n",
            "1\n",
            "tensor(0.9427, dtype=torch.float64)\n",
            "tensor(1.0000, dtype=torch.float64) tensor(1.0000, dtype=torch.float64)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "\n",
        "# Normalize class added at 10pm 12/12/2021\n",
        "class Normalize():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  # def __call__(self, sample):\n",
        "  #   inputs, targets = sample\n",
        "  #   inputs = transforms.Normalize(mean = 0.5, std = 0.5)\n",
        "  #   return inputs, targets\n",
        "  def __call__(self,vol):\n",
        "    vol =((vol-(vol.mean()))/vol.std()) + 1\n",
        "    return vol\n",
        "\n",
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(), transforms.Normalize(mean = 0.5, std = 0.5)] #added at 10:31pm 13/12/2021 to normalize the inputs\n",
        "# )\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), Normalize() ] #added at 11:00pm 13/12/2021 to normalize the inputs. THIS NORMALIZES to mean = 0 and std = -1\n",
        ")\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset) :\n",
        "  def __init__(self, annotations, img_dir, transform = transform, target_transform = None) :\n",
        "    self.img_labels = annotations\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self,idx) :\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels[idx][0] + \"-GTV-1.nii\" )\n",
        "    image_sitk = sitk.ReadImage(img_path)\n",
        "    image = sitk.GetArrayFromImage(image_sitk)\n",
        "    label = self.img_labels[idx][1]\n",
        "    if self.transform :\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform :\n",
        "      label = self.target_transform(label)\n",
        "    return image,label\n",
        "\n",
        "training_data = ImageDataset(outcomes_train, os.path.join(project_folder, \"Textured_Masks\"), transform = transforms.ToTensor())\n",
        "validation_data = ImageDataset(outcomes_validate, os.path.join(project_folder, \"Textured_Masks\"), transform = transforms.ToTensor())\n",
        "test_data = ImageDataset(outcomes_test, os.path.join(project_folder, \"Textured_Masks\"), transform = transforms.ToTensor())\n",
        "print(training_data[0][0][0][0][0])\n",
        "print(training_data[0][0].mean(), training_data[0][0].std())\n",
        "print(training_data[0][1])\n",
        "print(training_data[1][0][0][0][0])\n",
        "print(training_data[1][0].mean(), training_data[1][0].std())\n",
        "print(training_data[1][1])\n",
        "# print(len(training_data))\n",
        "\n",
        "training_data = ImageDataset(outcomes_train, os.path.join(project_folder, \"Textured_Masks\"), transform = transform)\n",
        "validation_data = ImageDataset(outcomes_validate, os.path.join(project_folder, \"Textured_Masks\"), transform = transform)\n",
        "test_data = ImageDataset(outcomes_test, os.path.join(project_folder, \"Textured_Masks\"), transform = transform) \n",
        "print(training_data[0][0][0][0][0])\n",
        "print(training_data[0][0].mean(), training_data[0][0].std())\n",
        "print(training_data[0][1])\n",
        "print(training_data[1][0][0][0][0])\n",
        "print(training_data[1][0].mean(), training_data[1][0].std())\n",
        "print(training_data[1][1])\n",
        "# print(len(training_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0KfrxkyK2-sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14567dc-49da-4f26-ce8a-5f75be8dcdc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size = 4, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 4, shuffle = False)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size = 4, shuffle = True)\n",
        "\n",
        "print(len(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k-4rwOY43AzL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1,4,2,2)\n",
        "        self.pool = nn.MaxPool3d(2,2)\n",
        "        self.conv2 = nn.Conv3d(4,16,2,2)\n",
        "        self.conv3 = nn.Conv3d(16,64,2,2)\n",
        "        self.conv4 = nn.Conv3d(64,256,2,2)\n",
        "        self.fc1 = nn.Linear(256,64)\n",
        "        self.fc2 = nn.Linear(64,16)\n",
        "        self.fc3 = nn.Linear(16,2)\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv3(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv4(x)))\n",
        "        x = x.view(-1, 256)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        # return F.leaky_relu(x)\n",
        "        return x\n",
        "\n",
        "# class CNN(nn.Module):   \n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "\n",
        "#         self.cnn_layers = nn.Sequential(\n",
        "#             # Defining a 2D convolution layer\n",
        "#             nn.Conv3d(1, 4, kernel_size=3, stride=3, padding=0),\n",
        "#             nn.BatchNorm3d(4),\n",
        "#             nn.LeakyReLU(inplace=True),\n",
        "#             nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "#             # Defining another 2D convolution layer\n",
        "#             nn.Conv3d(4, 16, kernel_size=3, stride=3, padding=0),\n",
        "#             nn.BatchNorm3d(16),\n",
        "#             nn.LeakyReLU(inplace=True),\n",
        "#             nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "#         )\n",
        "\n",
        "#         self.linear_layers = nn.Sequential(\n",
        "#             nn.Linear(5488, 2)\n",
        "#         )\n",
        "\n",
        "#     # Defining the forward pass    \n",
        "#     def forward(self, x):\n",
        "#         x = self.cnn_layers(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.linear_layers(x)\n",
        "#         return x\n",
        "  \n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tmjVAKQ3WVS",
        "outputId": "a380bf9a-3d87-41d2-f78d-91055b7383eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1      [4, 4, 132, 132, 132]              36\n",
            "         MaxPool3d-2         [4, 4, 66, 66, 66]               0\n",
            "            Conv3d-3        [4, 16, 33, 33, 33]             528\n",
            "         MaxPool3d-4        [4, 16, 16, 16, 16]               0\n",
            "            Conv3d-5           [4, 64, 8, 8, 8]           8,256\n",
            "         MaxPool3d-6           [4, 64, 4, 4, 4]               0\n",
            "            Conv3d-7          [4, 256, 2, 2, 2]         131,328\n",
            "         MaxPool3d-8          [4, 256, 1, 1, 1]               0\n",
            "            Linear-9                    [4, 64]          16,448\n",
            "           Linear-10                    [4, 16]           1,040\n",
            "           Linear-11                     [4, 2]              34\n",
            "================================================================\n",
            "Total params: 157,670\n",
            "Trainable params: 157,670\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 280.76\n",
            "Forward/backward pass size (MB): 336.60\n",
            "Params size (MB): 0.60\n",
            "Estimated Total Size (MB): 617.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (1,264,264,264), batch_size = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q8XutHXe3vOx"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "learning_rate = 0.0001\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqMifNHS4p7m",
        "outputId": "5443fe02-c662-4f1b-c220-06dccabb8288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for epoch 1\n",
            "=============================================\n",
            "Epoch 1/2, step 7/14, loss = 0.6933\n",
            "Epoch 1/2, step 14/14, loss = 0.5785\n",
            "avg train loss [0.162954]\n",
            "Training loss array at end of epoch 1: [9.125423729419708]. Total number of images used = 56\n",
            "Finished training for epoch 1\n",
            "Validation for epoch 1\n",
            "=============================================\n",
            "Accuracy on validation set for epoch 1 = 83.3%\n",
            "Loss on validation set = 1.8358817100524902\n",
            "Finished validation for epoch 1\n",
            "=============================================\n",
            "Training for epoch 2\n",
            "=============================================\n",
            "Epoch 2/2, step 7/14, loss = 0.7014\n",
            "Epoch 2/2, step 14/14, loss = 0.6124\n",
            "avg train loss [0.162954  0.1622121]\n",
            "Training loss array at end of epoch 2: [9.125423729419708, 9.083877801895142]. Total number of images used = 56\n",
            "Finished training for epoch 2\n",
            "Validation for epoch 2\n",
            "=============================================\n",
            "Accuracy on validation set for epoch 2 = 83.3%\n",
            "Loss on validation set = 1.81387859582901\n",
            "Finished validation for epoch 2\n",
            "=============================================\n",
            "FINISHED TRAINING\n",
            "All training batch losses = [0.6459228992462158, 0.6631618738174438, 0.6436505317687988, 0.6304388046264648, 0.7168189287185669, 0.6671513319015503, 0.6932774782180786, 0.5878269672393799, 0.6310144662857056, 0.6225243806838989, 0.7253493070602417, 0.6522467136383057, 0.6675170660018921, 0.5785229802131653, 0.6611536741256714, 0.6850553154945374, 0.6945734024047852, 0.5722141265869141, 0.5187925100326538, 0.618658721446991, 0.7013835906982422, 0.7551734447479248, 0.6489695906639099, 0.641074001789093, 0.5608007907867432, 0.7568560242652893, 0.6567873358726501, 0.6123852729797363]\n",
            "Training losses = [9.125423729419708, 9.083877801895142]\n",
            "Average training losses = [0.162954  0.1622121]\n",
            "Validation losses = [0.15299014 0.15115655]\n",
            "Accuracy on testing set = 66.7%\n"
          ]
        }
      ],
      "source": [
        "#training_loop\n",
        "num_epochs = 2\n",
        "n_total_steps = len(train_dataloader)\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "avg_train_loss = np.empty(0)\n",
        "avg_valid_loss = np.empty(0)\n",
        "all_training_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # ======================================= TRAINING LOOP ======================================\n",
        "  epoch_train_loss = 0 # will be used for plotting test vs valid loss curves\n",
        "  n_training_samples = 0\n",
        "\n",
        "  print(f'Training for epoch {epoch+1}')\n",
        "  print('=============================================')\n",
        "  \n",
        "  model.train()\n",
        "  for i, (images, labels) in enumerate(train_dataloader):\n",
        "    # Reformatting input images to have 5 dimensions and casting to a float\n",
        "    images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "    images = images.float()\n",
        "\n",
        "\n",
        "    # turning labels of size one to one-hot labels \n",
        "    # e.g labels = (1,0,0,1) --> hot_labels [(0,1), (1,0), (1,0), 0,1]\n",
        "    # This is need because nn.BCELogitsWithLoss() expects inputs of this format\n",
        "    hot_labels = torch.empty((images.shape[0], 2))\n",
        "    for index in range(len(labels)):\n",
        "          if labels[index] == 0 :\n",
        "            hot_labels[index,0] = 1\n",
        "            hot_labels[index,1] = 0\n",
        "          elif labels[index] == 1 :\n",
        "            hot_labels[index,0] = 0\n",
        "            hot_labels[index,1] = 1\n",
        "    \n",
        "    \n",
        "    # Send images and one-hot labels to the device (cuda/GPU)\n",
        "    images = images.to(device)\n",
        "    hot_labels = hot_labels.to(device)\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    # print(outputs)\n",
        "    loss = criterion(outputs, hot_labels)\n",
        "    \n",
        "\n",
        "    # Backwards pass\n",
        "    optimizer.zero_grad() # Clear gradients before \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the number of images in this batch to n_training_samples which will\n",
        "    # be used when calculating the average loss per image in the training set\n",
        "    n_training_samples += labels.shape[0]\n",
        "    #print(f'Number of samples completed after {i+1} batches = {n_training_samples}')\n",
        "    \n",
        "\n",
        "    # Updating the total training loss of this epoch\n",
        "    # Printing loss for current batch and the new updated total\n",
        "    # training loss for this epoch\n",
        "    #print(f'Loss of batch {i+1} = {loss.item():.2f}')\n",
        "    all_training_losses.append(loss.item())\n",
        "    epoch_train_loss += loss.item()\n",
        "    #print(f'Total training loss after batch {i+1} = {epoch_train_loss:.2f}')\n",
        "    \n",
        "\n",
        "    # Print a progress statement to ensure the network is running\n",
        "    if (i+1)%7 == 0 :\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "  \n",
        "  # Append the train_loss list with the total training loss for this epoch\n",
        "  train_loss.append(epoch_train_loss)\n",
        "\n",
        "  # Append the avg_train_loss list with the average training loss of this epoch\n",
        "  avg_train_loss = np.append(avg_train_loss, epoch_train_loss/n_training_samples)\n",
        "  print(f\"avg train loss {avg_train_loss}\")\n",
        "\n",
        "  print(f'Training loss array at end of epoch {epoch + 1}: {train_loss}. Total number of images used = {n_training_samples}')\n",
        "  print(f'Finished training for epoch {epoch+1}')\n",
        "\n",
        "  \n",
        "  \n",
        "  #================================================ VALIDATION LOOP =================================================\n",
        "  print(f'Validation for epoch {epoch+1}')\n",
        "  print('=============================================')\n",
        "  model.eval()\n",
        "  with torch.no_grad(): # ensuring gradients are not calculated during the validation loop\n",
        "    valid_epoch_loss = 0\n",
        "    n_valid_correct = 0\n",
        "    n_valid_samples = 0\n",
        "    for images, labels in validation_dataloader :\n",
        "      images = images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "      images = images.float()\n",
        "      hot_labels = torch.empty((images.shape[0], 2))\n",
        "      #print(new_labels.shape)\n",
        "      for index in range(len(labels)):\n",
        "          if labels[index] == 0 :\n",
        "            hot_labels[index,0] = 1\n",
        "            hot_labels[index,1] = 0\n",
        "          elif labels[index] == 1 :\n",
        "            hot_labels[index,0] = 0\n",
        "            hot_labels[index,1] = 1\n",
        "      images = images.to(device)\n",
        "      hot_labels = hot_labels.to(device)\n",
        "      outputs = model(images)\n",
        "      # calculate loss of validation set\n",
        "      loss = criterion(outputs, hot_labels)\n",
        "      valid_epoch_loss += loss.item()\n",
        "\n",
        "      # max returns (value, index) \n",
        "      _,predictions = torch.max(outputs, 1)\n",
        "      _,targets = torch.max(hot_labels, 1)\n",
        "      #print(f'predictions: {predictions}')\n",
        "      #print(f'targets: {targets}')\n",
        "      #print(f'correct in this batch: {(predictions == targets).sum().item()}')\n",
        "      n_valid_samples += labels.shape[0]\n",
        "      n_valid_correct += (predictions == targets).sum().item()\n",
        "      #print(f'n_correct = {n_correct}. n_samples = {n_samples}')\n",
        "    avg_valid_loss = np.append(avg_valid_loss, valid_epoch_loss/n_valid_samples)\n",
        "    #valid_loss.append(valid_epoch_loss)\n",
        "    acc = (100*n_valid_correct)/n_valid_samples\n",
        "    print(f'Accuracy on validation set for epoch {epoch+1} = {acc:.1f}%')\n",
        "    print(f'Loss on validation set = {valid_epoch_loss}')\n",
        "\n",
        "    print(f'Finished validation for epoch {epoch+1}')\n",
        "    print('=============================================')\n",
        "\n",
        "    #================================================ TENSORBOARD STUFF ===============================================\n",
        "  \n",
        "\n",
        "  tb.add_histogram(\"conv1.bias\", model.conv1.bias, epoch)\n",
        "  tb.add_histogram(\"conv1.weight\", model.conv1.weight, epoch)\n",
        "  tb.add_histogram(\"conv2.bias\", model.conv2.bias, epoch)\n",
        "  tb.add_histogram(\"conv2.weight\", model.conv2.weight, epoch)\n",
        "  tb.add_histogram(\"conv3.bias\", model.conv3.bias, epoch)\n",
        "  tb.add_histogram(\"conv3.weight\", model.conv3.weight, epoch)\n",
        "  tb.add_histogram(\"conv4.bias\", model.conv4.bias, epoch)\n",
        "  tb.add_histogram(\"conv4.weight\", model.conv4.weight, epoch)\n",
        "  tb.add_histogram(\"fc1.bias\", model.fc1.bias, epoch)\n",
        "  tb.add_histogram(\"fc1.weight\", model.fc1.weight, epoch)\n",
        "  tb.add_histogram(\"fc2.bias\", model.fc2.bias, epoch)\n",
        "  tb.add_histogram(\"fc2.weight\", model.fc2.weight, epoch)\n",
        "  tb.add_histogram(\"fc3.bias\", model.fc3.bias, epoch)\n",
        "  tb.add_histogram(\"fc3.weight\", model.fc3.weight, epoch)\n",
        "  tb.add_scalar(\"training_loss\", avg_train_loss[epoch], epoch)\n",
        "  tb.add_scalar(\"validation_loss\", avg_valid_loss[epoch], epoch)\n",
        "\n",
        "    # tb.add_scalar(\"Validation loss\", (valid_epoch_loss/n_valid_samples), epoch)\n",
        "\n",
        "# close tensorboard once the code has training loop has finished running\n",
        "# tb.close() \n",
        "\n",
        "print('FINISHED TRAINING')\n",
        "print(f'All training batch losses = {all_training_losses}')\n",
        "print(f'Training losses = {train_loss}')\n",
        "print(f'Average training losses = {avg_train_loss}')\n",
        "print(f'Validation losses = {avg_valid_loss}')\n",
        "#testing\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_dataloader :\n",
        "      images = images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "      images = images.float()\n",
        "      hot_labels = torch.empty((images.shape[0], 2))\n",
        "      #print(new_labels.shape)\n",
        "      for index in range(len(labels)):\n",
        "          if labels[index] == 0 :\n",
        "            hot_labels[index,0] = 1\n",
        "            hot_labels[index,1] = 0\n",
        "          elif labels[index] == 1 :\n",
        "            hot_labels[index,0] = 0\n",
        "            hot_labels[index,1] = 1\n",
        "      images = images.to(device)\n",
        "      hot_labels = hot_labels.to(device)\n",
        "      outputs = model(images)\n",
        "      # max returns (value, index) \n",
        "      _,predictions = torch.max(outputs, 1)\n",
        "      _,targets = torch.max(hot_labels,1)\n",
        "      #print(f'predictions: {predictions}')\n",
        "      #print(f'targets: {targets}')\n",
        "      n_samples += hot_labels.shape[0]\n",
        "      n_correct += (predictions == targets).sum().item()\n",
        "      #print(f'n_correct = {n_correct}. n_samples = {n_samples}')\n",
        "    \n",
        "    acc = (100*n_correct)/n_samples\n",
        "    print(f'Accuracy on testing set = {acc:.1f}%')\n",
        "\n",
        "\n",
        "# time renewal\n",
        "tb.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_valid_loss = []\n",
        "for i in range(len(valid_loss)):\n",
        "  avg_valid_loss.append(valid_loss[i]/n_valid_samples)\n",
        "\n",
        "print(n_valid_samples)\n",
        "print(avg_valid_loss)"
      ],
      "metadata": {
        "id": "qXqiU1y9fFMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = np.array(range(len(avg_train_loss))) + 1\n",
        "avg_train_loss_numpy = np.array(avg_train_loss)\n",
        "avg_valid_loss_numpy = np.array(avg_valid_loss)\n",
        "print(epochs)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(epochs,  avg_train_loss_numpy, label = 'Average training loss')\n",
        "plt.plot(epochs, avg_valid_loss_numpy, label = 'Average validation loss')\n",
        "#plt.title('Average losses: training vs validation')\n",
        "plt.ylabel('Average loss')\n",
        "plt.xlabel('Epoch number')\n",
        "plt.title(f'Losses over {len(epochs)} epochs with learning rate = {learning_rate}')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R22bWteAhpti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(test_dataloader))\n",
        "# print(data[0][0].numpy())\n",
        "array = data[0][0].numpy()\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "x,y,z = np.where(array > array[0][0][0])\n",
        "ax.scatter(x, y, z, c=z, alpha=1)\n",
        "print(len(array))\n",
        "\n",
        "ax.set_xlim(0,len(array))\n",
        "ax.set_ylim(0,len(array))\n",
        "ax.set_zlim(0,len(array))"
      ],
      "metadata": {
        "id": "HvtpX7g385yO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CNN4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}