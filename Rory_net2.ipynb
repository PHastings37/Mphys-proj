{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PHastings37/Mphys-proj/blob/main/Rory_net2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXfC9xFT0-UX",
        "outputId": "82f1d9c5-1dc4-4134-aaaa-6030fcd8e88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (1.10.0)\n",
            "Requirement already satisfied: torchvision in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (0.11.1)\n",
            "Requirement already satisfied: typing-extensions in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from torch) (4.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from torchvision) (1.21.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (4.5.5.62)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from opencv-contrib-python) (1.21.2)\n",
            "Requirement already satisfied: scikit-learn in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from scikit-learn) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (from scikit-learn) (1.21.2)\n",
            "Requirement already satisfied: SimpleITK in /home/phastings37/anaconda3/envs/python38/lib/python3.9/site-packages (2.1.1)\n",
            "Using cuda device\n",
            "/content/gdrive/My Drive/Mphys project/cancerdatasem2(40p).csv\n",
            "metadata_file path: /content/gdrive/My Drive/Mphys project/cancerdatasem2(40p).csv\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "/content/gdrive/My Drive/Mphys project/cancerdatasem2(40p).csv not found.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1849/4186290407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;31m#====================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m \u001b[0mpatient_IDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_markers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdead_statuses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0mdead_patient_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malive_patient_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatient_status_on_check_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_1849/4186290407.py\u001b[0m in \u001b[0;36mopen_metadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mmetadata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclinical_data_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'metadata_file path: {metadata_file}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Length of metadata array is {len(metadata)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/python38/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/python38/lib/python3.9/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/python38/lib/python3.9/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /content/gdrive/My Drive/Mphys project/cancerdatasem2(40p).csv not found."
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code trains, validates and tests a custom binary classfiying CNN.\n",
        "\n",
        "The inputs to the network are 264 x 264 x 264 textured masks of NSCLC pre-treatment CT scans.\n",
        "\n",
        "Rory Farwell and Patrick Hastings 08/02/2022\n",
        "\n",
        "\"\"\"\n",
        "#====================================================================\n",
        "#======================= IMPORTING FUNCTIONS ========================\n",
        "#====================================================================\n",
        "\n",
        "# Un hash below if on Google Colab\n",
        "!pip install torch torchvision\n",
        "!pip install opencv-contrib-python\n",
        "!pip install scikit-learn\n",
        "!pip install SimpleITK\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv3d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch import flatten\n",
        "from torch import nn\n",
        "from torch import reshape\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision.io import read_image\n",
        "from torch.optim import Adam\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "#====================================================================\n",
        "#=================== COLAB SPECIFIC CODE ============================\n",
        "#====================================================================\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "#====================================================================\n",
        "#=================== SELECT DEVICE ==================================\n",
        "#====================================================================\n",
        "\n",
        "# Connect to GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')\n",
        "# /content/gdrive/MyDrive/MPhys/Data/COLAB-Clinical-Data.csv\n",
        "# Specify project folder location\n",
        "project_folder = \"/content/gdrive/My Drive/Mphys project\"\n",
        "clinical_data_filename = \"cancerdatasem2(40p).csv\"\n",
        "print(os.path.join(project_folder, clinical_data_filename))\n",
        "\n",
        "#====================================================================\n",
        "#=================== DEFINING FUNCTIONS =============================\n",
        "#====================================================================\n",
        "\n",
        "def equalise_array_lengths(array_1, array_2) :\n",
        "  \"\"\"\n",
        "  This functions takes in the arguments of two lists and makes sure they are returned as the same length.\n",
        "\n",
        "  Rory Farwell 02/12/2021\n",
        "  \"\"\"\n",
        "  # output_array = []\n",
        "\n",
        "  if len(array_1) > len(array_2) :\n",
        "    array_1 = array_1[:len(array_2)]\n",
        "  elif len(array_1) < len(array_2) :\n",
        "    array_2 = array_2[:len(array_1)]\n",
        "\n",
        "  # array_1 = array_1[:20]\n",
        "  # array_2 = array_2[:20]\n",
        "\n",
        "  return (array_1, array_2)\n",
        "\n",
        "def remove_same_elements(small_array, long_array) :\n",
        "  \"\"\"\n",
        "  For use in the context, all the elements in small_array come from long_array.\n",
        "  This function will remove all of the elements used in small_array from_long_array.  \n",
        "  \"\"\"\n",
        "  for element in small_array :\n",
        "    long_array.remove(element)\n",
        "  return long_array\n",
        "\n",
        "def create_subgroup(input_array, original_array_length, desired_percentage) :\n",
        "  \"\"\"\n",
        "  This function outputs a subgroup array (e.g. training array) using a specified output array name,\n",
        "  input array and percentage length\n",
        "  \"\"\"\n",
        "  desired_length = int(original_array_length * desired_percentage)\n",
        "  output_array = random.sample(input_array, desired_length)\n",
        "  return output_array\n",
        "\n",
        "def open_metadata() :\n",
        "    \"\"\"\n",
        "    Opens the metadata file using the globall defined variables 'project_folder' and 'clinical_data_filename'.\n",
        "\n",
        "    Returns patient_IDs which will be used for checking the data is shuffled\n",
        "    Returns time_markers which will be used for checking patient status at a specified timepoint\n",
        "    Returns dead_statuses.\n",
        "\n",
        "    Rory Farwell and Patrick Hastings 08/02/2022\n",
        "    \"\"\"\n",
        "    metadata_file = os.path.join(project_folder, clinical_data_filename)\n",
        "    print(f'metadata_file path: {metadata_file}')\n",
        "    metadata = np.genfromtxt(metadata_file, comments = '%', dtype=\"str\", delimiter=\",\")\n",
        "    print(f\"Length of metadata array is {len(metadata)}\")\n",
        "\n",
        "    # Retrieve data from metadata file\n",
        "    patient_IDs = metadata[:,0] # selecting patient IDs from the csv file\n",
        "    time_markers = metadata[:,8] # selecting the day of the last patient review from the csv file\n",
        "    dead_statuses = metadata[:,9] # selecting the dead status on the last review day\n",
        "\n",
        "    time_markers = time_markers.astype(np.float32) # converting to float\n",
        "    dead_statuses = dead_statuses.astype(np.float32) # converting to float\n",
        "\n",
        "    return patient_IDs, time_markers, dead_statuses\n",
        "\n",
        "def patient_status_on_check_day(check_day) :\n",
        "    \"\"\"\n",
        "    Changes patient status according to patient status on the check day.\n",
        "    There are 4 possibilities here:\n",
        "    1. Dead: timepoint < check_day -> Dead on check day [1 -> 1]\n",
        "    2. Dead: timepoint > check_day -> Alive on check day [1 -> 0]\n",
        "    3. Alive: timepoint > check_day -> Alive on check day [0 -> 0]\n",
        "    4. Alive: timepoint < check_day -> no info on current status -> right-censored data\n",
        "\n",
        "    Rory Farwell and Patrick Hastings 08/02/2022\n",
        "    \"\"\"\n",
        "\n",
        "    dead_counter = 0\n",
        "    alive_counter = 0\n",
        "    no_info_counter = 0\n",
        "    dead_patient_array = []\n",
        "    alive_patient_array = []\n",
        "\n",
        "    for i in range(len(dead_statuses)) :\n",
        "        temp_patient_ID = patient_IDs[i]\n",
        "        temp_dead_status = dead_statuses[i]\n",
        "        temp_time_marker = time_markers[i]\n",
        "        if temp_dead_status == 1 : #if the patient is dead\n",
        "            if temp_time_marker < check_day :# situation 1\n",
        "                dead_patient_array.append([temp_patient_ID, 1])\n",
        "                dead_counter += 1\n",
        "                continue\n",
        "            elif temp_time_marker > check_day : # situation 2\n",
        "                alive_patient_array.append([temp_patient_ID, 0])\n",
        "                alive_counter += 1\n",
        "                continue\n",
        "        elif temp_dead_status == 0 : #if the patient is alive\n",
        "            if temp_time_marker < check_day : # situation 4\n",
        "                no_info_counter += 1\n",
        "                continue\n",
        "            elif temp_time_marker > check_day :\n",
        "                alive_patient_array.append([temp_patient_ID, 0])\n",
        "                alive_counter += 1\n",
        "                continue\n",
        "\n",
        "    # Printing the results of this loop (the number of dead and alive patients at the check day)\n",
        "    print(f\"Number of patients dead after {check_day} days: {dead_counter}\")\n",
        "    print(f\"Number of patients alive counter after {check_day} days: {alive_counter}\")\n",
        "    print(f\"Number of right-censored data when using a check day of {check_day} days: {no_info_counter}\")\n",
        "\n",
        "    return dead_patient_array, alive_patient_array\n",
        "\n",
        "def equalise_arrays(array_1, array_2) :\n",
        "    \"\"\"\n",
        "    Equalise the length arrays 1 and 2 to the length of the shortest of the two.\n",
        "\n",
        "    Rory Farwell and Patrick Hastings 08/02/2022\n",
        "    \"\"\"\n",
        "    new_array_1 = equalise_array_lengths(array_1, array_2)[0]\n",
        "    new_array_2 = equalise_array_lengths(array_1, array_2)[1]\n",
        "    print(f\"The alive and dead arrays have been sorted (randomly) so that they are both of length {len(new_array_1)}, {len(new_array_2)}\")\n",
        "\n",
        "    return new_array_1, new_array_2\n",
        "\n",
        "def create_final_datasets() :\n",
        "    \"\"\"\n",
        "    Combines the dead and alive arrays of each training, validation and testing data sets to produce the final lists.\n",
        "    And shuffles them.\n",
        "\n",
        "    Rory Farwell and Patrick Hastings 08/02/2022\n",
        "    \"\"\"\n",
        "    outcomes_train = train_patients_dead + train_patients_alive\n",
        "    outcomes_test = test_patients_dead + test_patients_alive\n",
        "    outcomes_validate = validate_patients_dead + validate_patients_alive\n",
        "    \n",
        "    random.shuffle(outcomes_train)\n",
        "    random.shuffle(outcomes_test)\n",
        "    random.shuffle(outcomes_validate)\n",
        "\n",
        "    print(f'Length of shuffled outcomes_train: {len(outcomes_train)}') \n",
        "    print(f'Length of shuffled outcomes_validate: {len(outcomes_validate)}')\n",
        "    print(f'Length of shuffled outcomes_test: {len(outcomes_test)}')\n",
        "\n",
        "    return outcomes_train, outcomes_validate, outcomes_test\n",
        "\n",
        "def convert_to_one_hot_labels(images, labels) :\n",
        "    \"\"\"\n",
        "    This function converts the labels to one-hot labels so that they will work with the BCEwithLogitsLoss\n",
        "    \"\"\"\n",
        "    hot_labels = torch.empty((images.shape[0], 2))\n",
        "    \n",
        "    for index in range(len(labels)) :\n",
        "        if labels[index] == 0 :\n",
        "            hot_labels[index,0] = 1\n",
        "            hot_labels[index,1] = 0\n",
        "        elif labels[index] == 1 :\n",
        "            hot_labels[index, 0] = 0\n",
        "            hot_labels[index, 1] = 1\n",
        "    \n",
        "    return hot_labels\n",
        "\n",
        "def training_loop():\n",
        "    epoch_train_loss = 0 # will be used for plotting testing vs validation loss curves\n",
        "    n_training_samples = 0\n",
        "    print(f'Training for epoch {epoch+1}')\n",
        "    print(\"=============================\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    \n",
        "\n",
        "    for i, (images, labels) in enumerate(train_dataloader):\n",
        "        images = reshape(images, (images.shape[0], 1, 264, 264, 264))\n",
        "        images = images.float()\n",
        "        \n",
        "        #writer.plot_batch(labels[i], images[i])\n",
        "        hot_labels = convert_to_one_hot_labels(images, labels)\n",
        "        \n",
        "        images = images.to(device)\n",
        "        hot_labels = hot_labels.to(device)\n",
        "\n",
        "        print(f\"tag:{patient}\")\n",
        "        writer.plot_tumour(patient[i], images[i])\n",
        "        \n",
        "\n",
        "        #forward pass\n",
        "        outputs = model(images)\n",
        "        # print (outputs)\n",
        "        loss = criterion(outputs, hot_labels)\n",
        "\n",
        "        #backwards pass\n",
        "        optimizer.zero_grad() #clears gradients before performing backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add the number of images in this batch to n_training_samples which will\n",
        "        # be used when calculating the average loss per image in the training set\n",
        "        n_training_samples += labels.shape[0]\n",
        "\n",
        "        # Updating the total training loss of this epoch\n",
        "        all_training_losses.append(loss.item())\n",
        "        epoch_train_loss += loss.item()\n",
        "        \n",
        "        if (i+1)%1 == 0 :\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "    # Append the train_loss list with the total training loss for this epoch\n",
        "    train_loss.append(epoch_train_loss)\n",
        "    \n",
        "\n",
        "    #Append the avg_train_loss list with the average training loss of this epoch\n",
        "    avg_train_loss = epoch_train_loss/n_training_samples\n",
        "    writer.add_scalar(\"training loss\", avg_train_loss, epoch)\n",
        "    print(f\"Average training loss list: {avg_train_loss}\")\n",
        "\n",
        "    print(f\"Training loss array at end of epoch {epoch + 1}: {train_loss}. Total number of images used = {n_training_samples}.\")\n",
        "    print(f\"Finished training for epoch {epoch + 1}\")\n",
        "\n",
        "    return avg_train_loss\n",
        "\n",
        "def validation_loop() :\n",
        "    print(f'Validation for epoch {epoch + 1}')\n",
        "    print('=================================')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): # ensuring gradients are not calculated during the validation loop\n",
        "        valid_epoch_loss = 0\n",
        "        n_valid_correct = 0\n",
        "        n_valid_samples = 0\n",
        "        for images, labels in validation_dataloader :\n",
        "            images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "            images = images.float()\n",
        "            hot_labels = convert_to_one_hot_labels(images, labels)\n",
        "\n",
        "            images = images.to(device)\n",
        "            hot_labels = hot_labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # calculate loss of validation set\n",
        "            loss = criterion(outputs, hot_labels)\n",
        "            valid_epoch_loss += loss.item()\n",
        "\n",
        "            # max returns (value, index) \n",
        "            _,predictions = torch.max(outputs, 1)\n",
        "            _,targets = torch.max(hot_labels, 1)\n",
        "            #print(f'predictions: {predictions}')\n",
        "            #print(f'targets: {targets}')\n",
        "            #print(f'correct in this batch: {(predictions == targets).sum().item()}')\n",
        "            n_valid_samples += labels.shape[0]\n",
        "            n_valid_correct += (predictions == targets).sum().item()\n",
        "            #print(f'n_correct = {n_correct}. n_samples = {n_samples}')\n",
        "        avg_valid_loss = valid_epoch_loss/n_valid_samples\n",
        "        #valid_loss.append(valid_epoch_loss)\n",
        "        acc = (100*n_valid_correct)/n_valid_samples\n",
        "        print(f'Accuracy on validation set for epoch {epoch+1} = {acc:.1f}%')\n",
        "        print(f'Loss on validation set = {valid_epoch_loss}')\n",
        "\n",
        "        print(f'Finished validation for epoch {epoch+1}')\n",
        "        print('=============================================')\n",
        "        writer.add_scalar(\"Validation Loss\", avg_valid_loss, epoch)\n",
        "        writer.add_scalar(\"Validation Accuracy\", acc, epoch)\n",
        "    return avg_valid_loss\n",
        "\n",
        "def testing_loop():\n",
        "  print(\"---- Currently testing the network on unseen data ----\")\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    counter = 0\n",
        "    for images, labels in test_dataloader :\n",
        "      # counter+=1\n",
        "      # print(counter)\n",
        "      images = images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "      images = images.float()\n",
        "      hot_labels = convert_to_one_hot_labels(images, labels)\n",
        "\n",
        "      images = images.to(device)\n",
        "      hot_labels = hot_labels.to(device)\n",
        "      outputs = model(images)\n",
        "      # max returns (value, index) \n",
        "      _,predictions = torch.max(outputs, 1)\n",
        "      _,targets = torch.max(hot_labels,1)\n",
        "      #print(f'predictions: {predictions}')\n",
        "      #print(f'targets: {targets}')\n",
        "      n_samples += hot_labels.shape[0]\n",
        "      n_correct += (predictions == targets).sum().item()\n",
        "      #print(f'n_correct = {n_correct}. n_samples = {n_samples}')\n",
        "    \n",
        "    acc = (100*n_correct)/n_samples\n",
        "\n",
        "    return acc\n",
        "\n",
        "def plot_loss_curves() :\n",
        "  new_avg_train_loss = avg_train_loss\n",
        "  new_avg_valid_loss = avg_valid_loss\n",
        "\n",
        "  epochs = np.array(range(num_epochs)) + 1\n",
        "  fig = plt.figure()\n",
        "  plt.xticks(fontsize=20)\n",
        "  plt.yticks(fontsize=20)\n",
        "  fig.set_size_inches(20, 10)\n",
        "  plt.plot(epochs, new_avg_train_loss, label = 'Average training loss',linewidth=7.0)\n",
        "  plt.plot(epochs, new_avg_valid_loss, label = 'Average validation loss',linewidth=7.0)\n",
        "  plt.legend(loc='best', prop={'size': 20})\n",
        "  plt.ylabel('Average Loss', fontsize = 20)\n",
        "  plt.xlabel('Epoch Number', fontsize = 20)\n",
        "  plt.show()\n",
        "  return\n",
        "#====================================================================\n",
        "#=================  CLASS DEFINITIONS ===============================\n",
        "#====================================================================\n",
        "\n",
        "\"\"\"\n",
        "Custom tensorboard writer class\n",
        "From Donal McSweeney\n",
        "\"\"\"\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class customWriter(SummaryWriter):\n",
        "    def __init__(self, log_dir, batch_size, epoch, num_classes):\n",
        "        super(customWriter, self).__init__()\n",
        "        self.log_dir = log_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.epoch = epoch\n",
        "        self.num_classes = num_classes\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.class_loss = {n: [] for n in range(num_classes+1)}\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1/(1+torch.exp(-x))\n",
        "\n",
        "    def reset_losses(self):\n",
        "        self.train_loss, self.val_loss, self.class_loss = [], [], {\n",
        "            n: [] for n in range(self.num_classes+1)}\n",
        "\n",
        "    def plot_batch(self, tag, images):\n",
        "        \"\"\"\n",
        "        Plot batches in grid\n",
        "​\n",
        "        Args: tag = identifier for plot (string)\n",
        "              images = input batch (torch.tensor)\n",
        "        \"\"\"\n",
        "        img_grid = torchvision.utils.make_grid(images, nrow=self.batch_size // 2)\n",
        "        self.add_image(tag, img_grid)\n",
        "\n",
        "    \n",
        "    def plot_tumour(self, tag, image):\n",
        "        \"\"\"\n",
        "        Plot predictions vs target segmentation.\n",
        "        Args: tag = identifier for plot (string)\n",
        "              prediction = batch output of trained model (torch.tensor)\n",
        "              target = batch ground-truth segmentations (torch.tensor)\n",
        "        \"\"\"\n",
        "        # fig = plt.figure(figsize=(24, 24))\n",
        "        # prediction = self.sigmoid(prediction)\n",
        "        # for idx in np.arange(self.batch_size):\n",
        "        #     ax = fig.add_subplot(self.batch_size // 2, self.batch_size // 2,\n",
        "        #                         idx+1, xticks=[], yticks=[], label='segmentations')\n",
        "        #     ax.imshow(prediction[idx, 0].cpu().numpy(\n",
        "        #     ), cmap='viridis')\n",
        "        #     if plot_target:\n",
        "        #         ax.imshow(target[idx, 0].cpu().numpy(), cmap='gray', alpha=0.25)\n",
        "        #     ax.set_title('prediction @ epoch: {} - idx: {}'.format(self.epoch, idx))\n",
        "        # self.add_figure(tag, fig)\n",
        "        fig = plt.figure(figsize=(24, 24))\n",
        "        \n",
        "        ax = fig.add_subplot()\n",
        "\n",
        "        \n",
        "        image=image.cpu()\n",
        "        \n",
        "        image=image.detach().numpy()\n",
        "        \n",
        "        \n",
        "        image=image[0,:,:,:]\n",
        "        \n",
        "        image=image[:,:,123]\n",
        "        \n",
        "        #print(f\"tag:{tag}\")\n",
        "        ax.imshow(image, cmap=\"viridis\")\n",
        "        ax.set_title(\"tumour\")\n",
        "        self.add_figure(tag, fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def plot_histogram(self, tag, prediction):\n",
        "        print('Plotting histogram')\n",
        "        fig = plt.figure(figsize=(24, 24))\n",
        "        for idx in np.arange(self.batch_size):\n",
        "            ax = fig.add_subplot(self.batch_size // 2, self.batch_size // 2,\n",
        "                                 idx+1, yticks=[], label='histogram')\n",
        "            pred_norm = (prediction[idx, 0]-prediction[idx, 0].min())/(\n",
        "                prediction[idx, 0].max()-prediction[idx, 0].min())\n",
        "            ax.hist(pred_norm.cpu().flatten(), bins=100)\n",
        "            ax.set_title(\n",
        "                f'Prediction histogram @ epoch: {self.epoch} - idx: {idx}')\n",
        "        self.add_figure(tag, fig)\n",
        "\n",
        "    def per_class_loss(self, prediction, target, criterion, alpha=None):\n",
        "        # Predict shape: (4, 1, 512, 512)\n",
        "        # Target shape: (4, 1, 512, 512)\n",
        "        #pred, target = prediction.cpu().numpy(), target.cpu().numpy()\n",
        "        pred, target = prediction, target\n",
        "        for class_ in range(self.num_classes + 1):\n",
        "            class_pred, class_tgt = torch.where(\n",
        "                target == class_, pred, torch.tensor([0], dtype=torch.float32).cuda()),  torch.where(target == class_, target, torch.tensor([0], dtype=torch.float32).cuda())\n",
        "\n",
        "            #class_pred, class_tgt = pred[target == class_], target[target == class_] \n",
        "            if alpha is not None:\n",
        "                loss = criterion(class_pred, class_tgt, alpha)\n",
        "                #bce_loss, dice_loss = criterion(class_pred, class_tgt, alpha)\n",
        "            else:\n",
        "                loss = criterion(class_pred, class_tgt)\n",
        "                #bce_loss, dice_loss = criterion(class_pred, class_tgt)\n",
        "            #loss = bce_loss + dice_loss\n",
        "            self.class_loss[class_].append(loss.item())\n",
        "\n",
        "    def write_class_loss(self):\n",
        "        for class_ in range(self.num_classes+1):\n",
        "            self.add_scalar(f'Per Class loss for class {class_}', np.mean(self.class_loss[class_]), self.epoch)\n",
        "\n",
        "# Normalize class added 12/12/2021\n",
        "class Normalize():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self,vol):\n",
        "    vol =((vol-(vol.mean()))/vol.std()) + 1\n",
        "    return vol\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), Normalize() ] #added 13/12/2021 to normalize the inputs. THIS NORMALIZES to mean = 0 and std = 1\n",
        ")\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset) :\n",
        "  def __init__(self, annotations, img_dir, transform = transform, target_transform = None) :\n",
        "    self.img_labels = annotations\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self,idx) :\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels[idx][0] + \"-GTV-1.nii\" )\n",
        "    image_sitk = sitk.ReadImage(img_path)\n",
        "    # ID = self.img_labels[idx][0]\n",
        "    # print(f'ID: {ID}')\n",
        "    image = sitk.GetArrayFromImage(image_sitk)\n",
        "    label = self.img_labels[idx][1]\n",
        "    if self.transform :\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform :\n",
        "      label = self.target_transform(label)\n",
        "    print(f\"patient ID: {patient_IDs[idx]}\")\n",
        "    global patient \n",
        "    patient = patient_IDs\n",
        "    return image,label\n",
        "\n",
        "class CNN(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "    def __init__(self):\n",
        "      super(CNN, self).__init__()\n",
        "      self.conv1 = nn.Conv3d(1,32,2,2)\n",
        "      self.pool = nn.MaxPool3d(2,2)\n",
        "      self.avg_pool = nn.AvgPool3d(4)\n",
        "      self.conv2 = nn.Conv3d(32,64,2,2)\n",
        "      self.conv3 = nn.Conv3d(64,128,2,2)\n",
        "      self.conv4 = nn.Conv3d(128,64,1,1)\n",
        "      self.conv5 = nn.Conv3d(64,16,1,1)\n",
        "      self.conv6 = nn.Conv3d(16,2,1,1)\n",
        "\n",
        "    # Defining the forward pass  (NIN method)  \n",
        "    def forward(self, x):\n",
        "        #print(f'Input to the network: {x}')\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv3(x)))\n",
        "        #print(f'Before the weird conv layers: {x}')\n",
        "        x = F.leaky_relu(self.conv4(x))\n",
        "        x = F.leaky_relu(self.conv5(x))\n",
        "        x = self.avg_pool(self.conv6(x))\n",
        "        #print(f'After the average pooling function: {x}')\n",
        "        x = x.view(-1,2)\n",
        "        \n",
        "        \n",
        "        return x\n",
        "        \n",
        "model = CNN().to(device) # Send the CNN to the device\n",
        "\n",
        "#====================================================================\n",
        "#=================== DEFIINING VARIABLES ============================\n",
        "#====================================================================\n",
        "\n",
        "check_day = 365 * 1.5 # This is defining the timeframe for which our CNN will consider the binary output (in days) \n",
        "\n",
        "# sanity check to check progress\n",
        "counter = 0 \n",
        "\n",
        "\n",
        "# Creating empty arrays that will be appended to later\n",
        "# These will contain the patient ID and dead status (on the check day).\n",
        "training_array = []\n",
        "testing_array = []\n",
        "validation_array = []\n",
        "\n",
        "#====================================================================\n",
        "#=================== HYPER PARAMETER DEFINITION =====================\n",
        "#====================================================================\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "num_epochs = 1\n",
        "\n",
        "#====================================================================\n",
        "#=================== MAIN CODE ======================================\n",
        "#====================================================================\n",
        "\n",
        "patient_IDs, time_markers, dead_statuses = open_metadata()\n",
        "dead_patient_array, alive_patient_array = patient_status_on_check_day(check_day)\n",
        "\n",
        "#  Shuffle both arrays to ensure a random selection of patient data which will be input to the CNN\n",
        "random.shuffle(dead_patient_array)\n",
        "random.shuffle(alive_patient_array)\n",
        "\n",
        "# Equalising the length of the 'dead' and 'alive' arrays so that we can ensure optimum training proportions\n",
        "new_dead_patient_array, new_alive_patient_array = equalise_arrays(dead_patient_array, alive_patient_array)\n",
        "equalised_array_length = len(new_alive_patient_array)\n",
        "\n",
        "train_patients_dead = create_subgroup(new_dead_patient_array, equalised_array_length, 0.7)\n",
        "train_patients_alive = create_subgroup(new_alive_patient_array, equalised_array_length, 0.7)\n",
        "\n",
        "new_dead_patient_array = remove_same_elements(train_patients_dead, new_dead_patient_array)\n",
        "new_alive_patient_array = remove_same_elements(train_patients_alive, new_alive_patient_array)\n",
        "\n",
        "test_patients_dead = create_subgroup(new_dead_patient_array, equalised_array_length, 0.15)\n",
        "test_patients_alive = create_subgroup(new_alive_patient_array, equalised_array_length, 0.15)\n",
        "\n",
        "new_dead_patient_array = remove_same_elements(test_patients_dead, new_dead_patient_array)\n",
        "new_alive_patient_array = remove_same_elements(test_patients_alive, new_alive_patient_array)\n",
        "\n",
        "validate_patients_dead = create_subgroup(new_dead_patient_array, equalised_array_length, 0.15)\n",
        "validate_patients_alive = create_subgroup(new_alive_patient_array, equalised_array_length, 0.15)\n",
        "\n",
        "new_dead_patient_array = remove_same_elements(validate_patients_dead, new_dead_patient_array)\n",
        "new_alive_patient_array = remove_same_elements(validate_patients_alive, new_alive_patient_array)\n",
        "\n",
        "print(f\"After separation into training, validation and testing arrays the number of unused data is {len(new_dead_patient_array) + len(new_alive_patient_array)}. If not then something has gone wrong.\")\n",
        "\n",
        "outcomes_train, outcomes_validate, outcomes_test = create_final_datasets()\n",
        "\n",
        "training_data = ImageDataset(outcomes_train, os.path.join(project_folder, \"Textured_Masks\"), transform = transform)\n",
        "validation_data = ImageDataset(outcomes_validate, os.path.join(project_folder, \"Textured_Masks\"), transform = transform)\n",
        "test_data = ImageDataset(outcomes_test, os.path.join(project_folder, \"Textured_Masks\"), transform = transform) \n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = 4, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 4, shuffle = False)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size = 4, shuffle = True)\n",
        "\n",
        "summary(model, (1,264,264,264), batch_size = 4)\n",
        "patient = \"\"\n",
        "\n",
        "#============================ TRAINING AND VALIDATION LOOP ==========\n",
        "writer = customWriter(project_folder, 4, 0, 1)\n",
        "writer.epoch = 0\n",
        "n_total_steps = len(train_dataloader)\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "avg_train_loss = np.empty(0)\n",
        "avg_valid_loss = np.empty(0)\n",
        "all_training_losses = []\n",
        "print(\"test\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    writer.epoch = epoch\n",
        "    avg_train_loss = np.append(avg_train_loss, training_loop())\n",
        "    avg_valid_loss = np.append(avg_valid_loss, validation_loop())\n",
        "\n",
        "print('FINISHED TRAINING')\n",
        "print(f'All training batch losses = {all_training_losses}')\n",
        "print(f'Training losses = {train_loss}')\n",
        "print(f'Average training losses = {avg_train_loss}')\n",
        "print(f'Validation losses = {avg_valid_loss}')\n",
        "\n",
        "#===================== PLOT LOSS CURVES =============================\n",
        "plot_loss_curves()\n",
        "\n",
        "#===================== TESTING LOOP =================================\n",
        "testing_accuracy = testing_loop()\n",
        "print(f'Accuracy on testing set = {testing_accuracy:.1f}%')\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "CleanCNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
