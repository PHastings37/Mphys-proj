{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTOwImDB+CAan3z9lvI1Aj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PHastings37/Mphys-proj/blob/main/CNN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4n0_Q-UQB4Y0",
        "outputId": "ff28fd1e-c2c6-47cb-cd3c-2ca2c5f752d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Using cpu device\n",
            "/content/gdrive/My Drive/Mphys project/cancerdatasem2.csv\n",
            "metadata_file path: /content/gdrive/My Drive/Mphys project/cancerdatasem2.csv\n",
            "Length of metadata array is 100\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1     [4, 32, 132, 132, 132]             288\n",
            "         MaxPool3d-2        [4, 32, 66, 66, 66]               0\n",
            "            Conv3d-3        [4, 64, 33, 33, 33]          16,448\n",
            "         MaxPool3d-4        [4, 64, 16, 16, 16]               0\n",
            "            Conv3d-5          [4, 128, 8, 8, 8]          65,664\n",
            "         MaxPool3d-6          [4, 128, 4, 4, 4]               0\n",
            "            Conv3d-7           [4, 64, 4, 4, 4]           8,256\n",
            "            Conv3d-8           [4, 16, 4, 4, 4]           1,040\n",
            "            Conv3d-9           [4, 11, 4, 4, 4]             187\n",
            "        AvgPool3d-10           [4, 11, 1, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 91,883\n",
            "Trainable params: 91,883\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 280.76\n",
            "Forward/backward pass size (MB): 2607.44\n",
            "Params size (MB): 0.35\n",
            "Estimated Total Size (MB): 2888.55\n",
            "----------------------------------------------------------------\n",
            "Training for epoch 1\n",
            "=============================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b69f5d8dd220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0mavg_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_valid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-b69f5d8dd220>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# print(hot_labels.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code trains, validates and tests a custom binary classfiying CNN.\n",
        "\n",
        "The inputs to the network are 264 x 264 x 264 textured masks of NSCLC pre-treatment CT scans.\n",
        "\n",
        "Rory Farwell and Patrick Hastings 08/02/2022\n",
        "\n",
        "\"\"\"\n",
        "#====================================================================\n",
        "#======================= IMPORTING FUNCTIONS ========================\n",
        "#====================================================================\n",
        "\n",
        "# Un hash below if on Google Colab\n",
        "!pip install torch torchvision\n",
        "!pip install opencv-contrib-python\n",
        "!pip install scikit-learn\n",
        "!pip install SimpleITK\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import SimpleITK as sitk\n",
        "import torch\n",
        "import math\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv3d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch import flatten\n",
        "from torch import nn\n",
        "from torch import reshape\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision.io import read_image\n",
        "from torch.optim import Adam\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary\n",
        "\n",
        "from scipy.ndimage import zoom, rotate\n",
        "\n",
        "#====================================================================\n",
        "#=================== COLAB SPECIFIC CODE ============================\n",
        "#====================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#====================================================================\n",
        "#=================== SELECT DEVICE ==================================\n",
        "#====================================================================\n",
        "\n",
        "# Connect to GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device} device')\n",
        "# /content/gdrive/MyDrive/MPhys/Data/COLAB-Clinical-Data.csv\n",
        "# Specify project folder location\n",
        "project_folder = \"/content/gdrive/My Drive/Mphys project\"\n",
        "clinical_data_filename = \"cancerdatasem2.csv\"\n",
        "print(os.path.join(project_folder, clinical_data_filename))\n",
        "\n",
        "#====================================================================\n",
        "#=================== DEFINING FUNCTIONS =============================\n",
        "#====================================================================\n",
        "\n",
        "def open_metadata() :\n",
        "    \"\"\"\n",
        "    Opens the metadata file using the globall defined variables 'project_folder' and 'clinical_data_filename'.\n",
        "\n",
        "    Returns patient_IDs which will be used for checking the data is shuffled\n",
        "    Returns time_markers which will be used for checking patient status at a specified timepoint\n",
        "    Returns dead_statuses.\n",
        "\n",
        "    Rory Farwell and Patrick Hastings 08/02/2022\n",
        "    \"\"\"\n",
        "    metadata_file = os.path.join(project_folder, clinical_data_filename)\n",
        "    print(f'metadata_file path: {metadata_file}')\n",
        "    metadata = np.genfromtxt(metadata_file, comments = '%', dtype=\"str\", delimiter=\",\")\n",
        "    print(f\"Length of metadata array is {len(metadata)}\")\n",
        "\n",
        "    # Retrieve data from metadata file\n",
        "    patient_IDs = metadata[:,0] # selecting patient IDs from the csv file\n",
        "    time_markers = metadata[:,8] # selecting the day of the last patient review from the csv file\n",
        "    dead_statuses = metadata[:,9] # selecting the dead status on the last review day\n",
        "\n",
        "    time_markers = time_markers.astype(np.float32) # converting to float\n",
        "    dead_statuses = dead_statuses.astype(np.float32) # converting to float\n",
        "\n",
        "    return patient_IDs, time_markers, dead_statuses\n",
        "\n",
        "def patient_label() :\n",
        "    \"\"\"\n",
        "    Changes patient status according to patient status on the check day.\n",
        "\n",
        "    here are the possible patient outcomes:\n",
        "    1. Dead t=0 < time_to_death < incriment1   label = 0\n",
        "    2. Dead incriment1 < time_to_death < incriment2   label = 1\n",
        "    and so on up to 5 years, where at which point patient will be declared alive.\n",
        "    Further to this patients who are right-censored will be counted as alive.\n",
        "\n",
        "    Rory Farwell and Patrick Hastings 08/02/2022\n",
        "    \"\"\"\n",
        "\n",
        "    dead_counter = 0\n",
        "    alive_counter = 0\n",
        "    no_info_counter = 0\n",
        "    labels = np.empty(100)\n",
        "\n",
        "    for i in range(len(dead_statuses)):\n",
        "\n",
        "      if dead_statuses[i] == 1 and time_markers[i] < 1800: #check patient is dead before 1800 days(5 years)\n",
        "        labels[i] = math.ceil(time_markers[i] / 180)\n",
        "      else:\n",
        "        \n",
        "        labels[i] = 11\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_loop():\n",
        "    epoch_train_loss = 0 # will be used for plotting testing vs validation loss curves\n",
        "    n_training_samples = 0\n",
        "    print(f'Training for epoch {epoch+1}')\n",
        "    print(\"=============================\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_dataloader):\n",
        "        images = reshape(images, (images.shape[0], 1, 264, 264, 264))\n",
        "        images = images.float()\n",
        "\n",
        "        hot_labels = convert_to_one_hot_labels(images, labels)\n",
        "        \n",
        "        images = images.to(device)\n",
        "        hot_labels = hot_labels.to(device)\n",
        "\n",
        "        # print(hot_labels.size())\n",
        "        # print(hot_labels)\n",
        "\n",
        "   \n",
        "\n",
        "        #forward pass\n",
        "        outputs = model(images)\n",
        "        # print (outputs)\n",
        "        loss = criterion(outputs, hot_labels)\n",
        "\n",
        "        #backwards pass\n",
        "        optimizer.zero_grad() #clears gradients before performing backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add the number of images in this batch to n_training_samples which will\n",
        "        # be used when calculating the average loss per image in the training set\n",
        "        n_training_samples += labels.shape[0]\n",
        "\n",
        "        # Updating the total training loss of this epoch\n",
        "        all_training_losses.append(loss.item())\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        if (i+1)%1 == 0 :\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "    # Append the train_loss list with the total training loss for this epoch\n",
        "    train_loss.append(epoch_train_loss)\n",
        "\n",
        "    #Append the avg_train_loss list with the average training loss of this epoch\n",
        "    avg_train_loss = epoch_train_loss/n_training_samples\n",
        "    print(f\"Average training loss list: {avg_train_loss}\")\n",
        "\n",
        "    print(f\"Training loss array at end of epoch {epoch + 1}: {train_loss}. Total number of images used = {n_training_samples}.\")\n",
        "    print(f\"Finished training for epoch {epoch + 1}\")\n",
        "\n",
        "    return avg_train_loss\n",
        "\n",
        "def validation_loop() :\n",
        "    print(f'Validation for epoch {epoch + 1}')\n",
        "    print('=================================')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad(): # ensuring gradients are not calculated during the validation loop\n",
        "        valid_epoch_loss = 0\n",
        "        n_valid_correct = 0\n",
        "        n_valid_samples = 0\n",
        "        for images, labels in validation_dataloader :\n",
        "            images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "            images = images.float()\n",
        "            hot_labels = convert_to_one_hot_labels(images, labels)\n",
        "\n",
        "            images = images.to(device)\n",
        "            hot_labels = hot_labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # calculate loss of validation set\n",
        "            loss = criterion(outputs, hot_labels)\n",
        "            valid_epoch_loss += loss.item()\n",
        "\n",
        "            # max returns (value, index) \n",
        "            _,predictions = torch.max(outputs, 1)\n",
        "            _,targets = torch.max(hot_labels, 1)\n",
        "            #print(f'predictions: {predictions}')\n",
        "            #print(f'targets: {targets}')\n",
        "            #print(f'correct in this batch: {(predictions == targets).sum().item()}')\n",
        "            n_valid_samples += labels.shape[0]\n",
        "            n_valid_correct += (predictions == targets).sum().item()\n",
        "            #print(f'n_correct = {n_correct}. n_samples = {n_samples}')\n",
        "        avg_valid_loss = valid_epoch_loss/n_valid_samples\n",
        "        #valid_loss.append(valid_epoch_loss)\n",
        "        acc = (100*n_valid_correct)/n_valid_samples\n",
        "        print(f'Accuracy on validation set for epoch {epoch+1} = {acc:.1f}%')\n",
        "        print(f'Loss on validation set = {valid_epoch_loss}')\n",
        "\n",
        "        print(f'Finished validation for epoch {epoch+1}')\n",
        "        print('=============================================')\n",
        "    return avg_valid_loss\n",
        "\n",
        "def testing_loop():\n",
        "  print(\"---- Currently testing the network on unseen data ----\")\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    counter = 0\n",
        "    for images, labels in test_dataloader :\n",
        "      # counter+=1\n",
        "      # print(counter)\n",
        "      images = images = reshape(images, (images.shape[0],1 ,264,264,264))\n",
        "      images = images.float()\n",
        "      hot_labels = convert_to_one_hot_labels(images, labels)\n",
        "\n",
        "      images = images.to(device)\n",
        "      hot_labels = hot_labels.to(device)\n",
        "      outputs = model(images)\n",
        "      # max returns (value, index) \n",
        "      _,predictions = torch.max(outputs, 1)\n",
        "      _,targets = torch.max(hot_labels,1)\n",
        "      #print(f'predictions: {predictions}')\n",
        "      #print(f'targets: {targets}')\n",
        "      n_samples += hot_labels.shape[0]\n",
        "      n_correct += (predictions == targets).sum().item()\n",
        "      #print(f'n_correct = {n_correct}. n_samples = {n_samples}')\n",
        "    \n",
        "    acc = (100*n_correct)/n_samples\n",
        "\n",
        "    return acc\n",
        "\n",
        "def plot_loss_curves() :\n",
        "  new_avg_train_loss = avg_train_loss\n",
        "  new_avg_valid_loss = avg_valid_loss\n",
        "\n",
        "  epochs = np.array(range(num_epochs)) + 1\n",
        "  fig = plt.figure()\n",
        "  plt.xticks(fontsize=20)\n",
        "  plt.yticks(fontsize=20)\n",
        "  fig.set_size_inches(20, 10)\n",
        "  plt.plot(epochs, new_avg_train_loss, label = 'Average training loss',linewidth=7.0)\n",
        "  plt.plot(epochs, new_avg_valid_loss, label = 'Average validation loss',linewidth=7.0)\n",
        "  plt.legend(loc='best', prop={'size': 20})\n",
        "  plt.ylabel('Average Loss', fontsize = 20)\n",
        "  plt.xlabel('Epoch Number', fontsize = 20)\n",
        "  plt.show()\n",
        "  return\n",
        "\n",
        "def window_and_level(image, level = 700, window = 1000) :\n",
        "  maxval = level + window/2\n",
        "  minval = level - window/2\n",
        "  wld = np.clip(image, minval, maxval)\n",
        "  wld -=minval\n",
        "  wld *= 1/window\n",
        "  return wld\n",
        "\n",
        "\n",
        "#====================================================================\n",
        "#=================  CLASS DEFINITIONS ===============================\n",
        "#====================================================================\n",
        "\n",
        "# Normalize class added 12/12/2021\n",
        "class Normalize():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self,vol):\n",
        "    vol =((vol-(vol.mean()))/vol.std()) + 1\n",
        "    return vol\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()] #added 13/12/2021 to normalize the inputs. THIS NORMALIZES to mean = 0 and std = 1\n",
        ")\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset) :\n",
        "  def __init__(self, annotations, img_dir, transform = transform, target_transform = None, shift_augment = True, rotate_augment = True, scale_augment = True, flip_augment = True) :\n",
        "    self.img_labels = annotations\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.shifts = shift_augment\n",
        "    self.rotations = rotate_augment\n",
        "    self.flips = flip_augment\n",
        "    self.scales = scale_augment\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self,idx) :\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels[idx][0] + \"-GTV-1.nii\" )\n",
        "    image_sitk = sitk.ReadImage(img_path)\n",
        "    # ID = self.img_labels[idx][0]\n",
        "    # print(f'ID: {ID}')\n",
        "    image = sitk.GetArrayFromImage(image_sitk)\n",
        "    label = self.img_labels[idx][1]\n",
        "\n",
        "    # Augmentations\n",
        "    if self.shifts:\n",
        "      mx_x, mx_yz = 10, 10 \n",
        "      # find shift values\n",
        "      cc_shift, ap_shift, lr_shift = random.randint(-mx_x,mx_x), random.randint(-mx_yz,mx_yz), random.randint(-mx_yz,mx_yz)\n",
        "      # pad for shifting into\n",
        "      image = np.pad(image, pad_width=((mx_x,mx_x),(mx_yz,mx_yz),(mx_yz,mx_yz)), mode='constant', constant_values=0)\n",
        "      # crop to complete shift\n",
        "      image = image[mx_x+cc_shift:264+mx_x+cc_shift, mx_yz+ap_shift:264+mx_yz+ap_shift, mx_yz+lr_shift:264+mx_yz+lr_shift]\n",
        "\n",
        "    if self.rotations and random.random() < 0.5 :\n",
        "      roll_angle = np.clip(np.random.normal(loc=0,scale=3), -10, 10)\n",
        "      image = self.rotation(image, roll_angle, rotation_plane=(1,2))\n",
        "\n",
        "    if self.scales and random.random() < 0.5 :\n",
        "      # same here -> zoom between 80-120%\n",
        "      scale_factor = np.clip(np.random.normal(loc=1.0,scale=0.05), 0.8, 1.2)\n",
        "      image = self.scale(image, scale_factor)\n",
        "    \n",
        "    image = window_and_level(image)\n",
        "\n",
        "    if self.transform :\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform :\n",
        "      label = self.target_transform(label)\n",
        "    return image,label\n",
        "  \n",
        "  def rotation(self, image, rotation_angle, rotation_plane):\n",
        "      # rotate the image or mask using scipy rotate function\n",
        "      order, cval = (3, -1024)\n",
        "      return rotate(input=image, angle=rotation_angle, axes=rotation_plane, reshape=False, order=order, mode='constant', cval=cval)\n",
        "    \n",
        "  def scale(self, image, scale_factor):\n",
        "      # scale the image or mask using scipy zoom function\n",
        "      order, cval = (3, -1024)\n",
        "      height, width, depth = image.shape\n",
        "      zheight = int(np.round(scale_factor*height))\n",
        "      zwidth = int(np.round(scale_factor*width))\n",
        "      zdepth = int(np.round(scale_factor*depth))\n",
        "      # zoomed out\n",
        "      if scale_factor < 1.0:\n",
        "          new_image = np.full_like(image, cval)\n",
        "          ud_buffer = (height-zheight) // 2\n",
        "          ap_buffer = (width-zwidth) // 2\n",
        "          lr_buffer = (depth-zdepth) // 2\n",
        "          new_image[ud_buffer:ud_buffer+zheight, ap_buffer:ap_buffer+zwidth, lr_buffer:lr_buffer+zdepth] = zoom(input=image, zoom=scale_factor, order=order, mode='constant', cval=cval)[0:zheight, 0:zwidth, 0:zdepth]\n",
        "          return new_image\n",
        "      elif scale_factor > 1.0:\n",
        "          new_image = zoom(input=image, zoom=scale_factor, order=order, mode='constant', cval=cval)[0:zheight, 0:zwidth, 0:zdepth]\n",
        "          ud_extra = (new_image.shape[0] - height) // 2\n",
        "          ap_extra = (new_image.shape[1] - width) // 2\n",
        "          lr_extra = (new_image.shape[2] - depth) // 2\n",
        "          new_image = new_image[ud_extra:ud_extra+height, ap_extra:ap_extra+width, lr_extra:lr_extra+depth]\n",
        "          return new_image\n",
        "      return image\n",
        "\n",
        "class CNN(nn.Module):   \n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # self.conv1 = nn.Conv3d(1,4,2,2)\n",
        "        # self.pool = nn.MaxPool3d(2,2)\n",
        "        # self.avg_pool = nn.AvgPool3d(2)\n",
        "        # self.conv2 = nn.Conv3d(4,16,2,2)\n",
        "        # self.conv3 = nn.Conv3d(16,64,2,2)\n",
        "        # self.conv4 = nn.Conv3d(64,256,2,2)\n",
        "        # self.dropout = nn.Dropout(0.25)\n",
        "        # self.fc1 = nn.Linear(256,64)\n",
        "        # self.fc2 = nn.Linear(64,16)\n",
        "        # self.fc3 = nn.Linear(16,2)\n",
        "\n",
        "    # def __init__(self):\n",
        "    #     super(CNN, self).__init__()\n",
        "        # self.conv1 = nn.Conv3d(1,16,2,2)\n",
        "        # self.pool = nn.MaxPool3d(2,2)\n",
        "        # self.avg_pool = nn.AvgPool3d(2)\n",
        "        # self.conv2 = nn.Conv3d(16,16,2,2)\n",
        "        # self.conv3 = nn.Conv3d(16,16,2,2)\n",
        "        # self.conv4 = nn.Conv3d(16,8,2,2)\n",
        "        # self.dropout = nn.Dropout(0.25)\n",
        "        # self.fc1 = nn.Linear(16,64)\n",
        "        # self.fc2 = nn.Linear(64,16)\n",
        "        # self.fc3 = nn.Linear(16,2)\n",
        "\n",
        "    def __init__(self):\n",
        "      super(CNN, self).__init__()\n",
        "      self.conv1 = nn.Conv3d(1,32,2,2)\n",
        "      self.pool = nn.MaxPool3d(2,2)\n",
        "      self.avg_pool = nn.AvgPool3d(4)\n",
        "      self.conv2 = nn.Conv3d(32,64,2,2)\n",
        "      self.conv3 = nn.Conv3d(64,128,2,2)\n",
        "      self.conv4 = nn.Conv3d(128,64,1,1)\n",
        "      self.conv5 = nn.Conv3d(64,16,1,1)\n",
        "      self.conv6 = nn.Conv3d(16,11,1,1)\n",
        "\n",
        "    # Defining the forward pass    (original)\n",
        "    # def forward(self, x):\n",
        "    #     print(f'Input to the network: {x}')\n",
        "    #     x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "    #     x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "    #     x = self.pool(F.leaky_relu(self.conv3(x)))\n",
        "    #     x = self.avg_pool(F.leaky_relu(self.conv4(x)))\n",
        "    #     print(f'After average pooling layer: {x}')\n",
        "    #     x = x.view(-1, 16)\n",
        "    #     print(f'After flattening: {x}')\n",
        "    #     x = self.dropout(x)\n",
        "    #     x = F.leaky_relu(self.fc1(x))\n",
        "    #     x = self.dropout(x)\n",
        "    #     x = F.leaky_relu(self.fc2(x))\n",
        "        \n",
        "    #     x = self.dropout(x)\n",
        "        \n",
        "    #     x = self.fc3(x)\n",
        "    #     print(x)\n",
        "    #     return x\n",
        "\n",
        "    # Defining the forward pass  (NIN method)  \n",
        "    def forward(self, x):\n",
        "        #print(f'Input to the network: {x}')\n",
        "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
        "        x = self.pool(F.leaky_relu(self.conv3(x)))\n",
        "        #print(f'Before the weird conv layers: {x}')\n",
        "        x = F.leaky_relu(self.conv4(x))\n",
        "        x = F.leaky_relu(self.conv5(x))\n",
        "        x = self.avg_pool(self.conv6(x))\n",
        "        #print(f'After the average pooling function: {x}')\n",
        "        x = x.view(-1,11)\n",
        "        \n",
        "        \n",
        "        return x\n",
        "        \n",
        "model = CNN().to(device) # Send the CNN to the device\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#====================================================================\n",
        "#=================== DEFIINING VARIABLES ============================\n",
        "#====================================================================\n",
        "\n",
        "\n",
        "# sanity check to check progress\n",
        "counter = 0 \n",
        "\n",
        "\n",
        "# Creating empty arrays that will be appended to later\n",
        "# These will contain the patient ID and dead status (on the check day).\n",
        "training_array = []\n",
        "testing_array = []\n",
        "validation_array = []\n",
        "\n",
        "#====================================================================\n",
        "#=================== HYPER PARAMETER DEFINITION =====================\n",
        "#====================================================================\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "num_epochs = 8\n",
        "\n",
        "#====================================================================\n",
        "#=================== MAIN CODE ======================================\n",
        "#====================================================================\n",
        "\n",
        "patient_labels = []\n",
        "patient_IDs, time_markers, dead_statuses = open_metadata()#gathers patient data\n",
        "patient_labels = patient_label()#assigns each patients a label based on survial time\n",
        "\n",
        "patient_labels = np.reshape(patient_labels, [100, 1])\n",
        "patient_IDs = np.reshape(patient_IDs, [100, 1])\n",
        "comb_array = np.hstack((patient_IDs, patient_labels))#creates one 2d array where each row is a patient and its corresponding label\n",
        "comb_array = comb_array.tolist()\n",
        "\n",
        "\n",
        "\n",
        "plt.hist(patient_labels, bins = 11)\n",
        "plt.show()\n",
        "\n",
        "full_dataset = ImageDataset(comb_array, os.path.join(project_folder, \"Textured_Masks\"), transform = transform)\n",
        "\n",
        "training_data, validation_data, test_data = torch.utils.data.random_split(full_dataset, [70,15,15])\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = 4, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 4, shuffle = False)\n",
        "validation_dataloader = DataLoader(validation_data, batch_size = 4, shuffle = True)\n",
        "\n",
        "summary(model, (1,264,264,264), batch_size = 4)\n",
        "\n",
        "#============================ TRAINING AND VALIDATION LOOP ==========\n",
        "n_total_steps = len(train_dataloader)\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "avg_train_loss = np.empty(0)\n",
        "avg_valid_loss = np.empty(0)\n",
        "all_training_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    avg_train_loss = np.append(avg_train_loss, training_loop())\n",
        "    avg_valid_loss = np.append(avg_valid_loss, validation_loop())\n",
        "\n",
        "print('FINISHED TRAINING')\n",
        "print(f'All training batch losses = {all_training_losses}')\n",
        "print(f'Training losses = {train_loss}')\n",
        "print(f'Average training losses = {avg_train_loss}')\n",
        "print(f'Validation losses = {avg_valid_loss}')\n",
        "\n",
        "#===================== PLOT LOSS CURVES =============================\n",
        "plot_loss_curves()\n",
        "\n",
        "#===================== TESTING LOOP =================================\n",
        "testing_accuracy = testing_loop()\n",
        "print(f'Accuracy on testing set = {testing_accuracy:.1f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_labels = np.reshape(patient_labels, [100, 1])\n",
        "patient_IDs = np.reshape(patient_IDs, [100, 1])\n",
        "\n",
        "comb_array = np.hstack((patient_IDs, patient_labels))"
      ],
      "metadata": {
        "id": "1txDKuB5bkys"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainset)"
      ],
      "metadata": {
        "id": "f5laySRbcF2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}